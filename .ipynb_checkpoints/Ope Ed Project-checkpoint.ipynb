{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Michael McCormack***\n",
    "\n",
    "In the Week 04 file folder you'll find the anonymous New York op ed from September 5, 2018, together with samples of writings of the main suspects. \n",
    "\n",
    "Use any tools you like to figure out who wrote the op ed. If you can find more writings of these people, please send them my way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How I approached this problem - \n",
    "\n",
    "Let me first say that this is a incredibly interesting problem that demonstrates a very relevant application of NLP.  I had a lot of fun doing this project. \n",
    "\n",
    "I began by doing some exploring on the internet.  I came across this idea of determining author based on sentence structure.  Below is an example of this idea used in a Kaggle competition.\n",
    "\n",
    "https://www.kaggle.com/christopher22/stylometry-identify-authors-by-sentence-structure/notebook\n",
    "\n",
    "Basically the idea is that you break down each training document by sentence and add a label for the author of each sentence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start off by reading in the training files into pandas DataFrames.  I then add a column indicating the author Next I concatenate all the dataframes together so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dan Coats.txt', 'r+') as in_file:\n",
    "    textDan = in_file.read()\n",
    "    sentsDan = nltk.sent_tokenize(textDan)\n",
    "    \n",
    "pdDan = pd.DataFrame({'sentence':sentsDan})\n",
    "pdDan['author'] = 'Dan Coats'\n",
    "\n",
    "\n",
    "with open('James Mattis.txt', 'r+') as in_file:\n",
    "    textMattis = in_file.read()\n",
    "    sentsMattis = nltk.sent_tokenize(textMattis)\n",
    "    \n",
    "pdMattis = pd.DataFrame({'sentence':sentsMattis})\n",
    "pdMattis['author'] = 'James Mattis'\n",
    "\n",
    "\n",
    "with open('John Kelly.txt', 'r+') as in_file:\n",
    "    textKelly = in_file.read()\n",
    "    sentsKelly = nltk.sent_tokenize(textKelly)\n",
    "    \n",
    "pdKelly = pd.DataFrame({'sentence':sentsKelly})\n",
    "pdKelly['author'] = 'John Kelly'\n",
    "\n",
    "\n",
    "with open('Kevin Hassett.txt', 'r+') as in_file:\n",
    "    textHassett = in_file.read()\n",
    "    sentsHassett = nltk.sent_tokenize(textHassett)\n",
    "    \n",
    "pdHassett = pd.DataFrame({'sentence':sentsHassett})\n",
    "pdHassett['author'] = 'Kevin Hassett'\n",
    "\n",
    "with open('Kirstjen Nielsen.txt', 'r+') as in_file:\n",
    "    textNielsen = in_file.read()\n",
    "    sentsNielsen = nltk.sent_tokenize(textNielsen)\n",
    "    \n",
    "pdNielsen = pd.DataFrame({'sentence':sentsNielsen})\n",
    "pdNielsen['author'] = 'Kirstjen Nielsen'\n",
    "\n",
    "with open('Larry Kudlow.txt', 'r+') as in_file:\n",
    "    textKudlow = in_file.read()\n",
    "    sentsKudlow = nltk.sent_tokenize(textKudlow)\n",
    "    \n",
    "pdKudlow = pd.DataFrame({'sentence':sentsKudlow})\n",
    "pdKudlow['author'] = 'Larry Kudlow'\n",
    "\n",
    "\n",
    "with open('Mike Pence.txt', 'r+') as in_file:\n",
    "    textPence = in_file.read()\n",
    "    sentsPence = nltk.sent_tokenize(textPence)\n",
    "    \n",
    "pdPence = pd.DataFrame({'sentence':sentsPence})\n",
    "pdPence['author'] = 'Mike Pence'\n",
    "\n",
    "with open('Mike Pompeo.txt', 'r+') as in_file:\n",
    "    textPompeo = in_file.read()\n",
    "    sentsPompeo = nltk.sent_tokenize(textPompeo)\n",
    "    \n",
    "pdPompeo = pd.DataFrame({'sentence':sentsPompeo})\n",
    "pdPompeo['author'] = 'Mike Pompeo'\n",
    "\n",
    "\n",
    "\n",
    "train = pd.DataFrame()\n",
    "train = pd.concat([pdDan, pdMattis,pdKelly,pdHassett,pdNielsen,pdKudlow,pdPence,pdPompeo])\n",
    "\n",
    "author_to_num ={'Dan Coats': \"Chicken\", 'James Mattis': \"Punk\", \n",
    "                'John Kelly': \"perp\", 'Kevin Hassett': \"Garbage\",\n",
    "                  'Kirstjen Nielsen': \"Scum\", 'Larry Kudlow': \"Toilet\", \n",
    "                'Mike Pence': \"Poop\", 'Mike Pompeo': \"Yuck\"}\n",
    "\n",
    "train[\"author\"].replace(author_to_num, inplace=True)\n",
    "train[\"sentence\"].replace('\\xa0', ' ', inplace=True)  # get rid of non-breaking spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.text = (train['text']\n",
    "#             .apply(sent_tokenize)\n",
    "#             .apply(pd.Series)\n",
    "#         #     .stack()\n",
    "             )\n",
    "#     #.to_frame('text')\n",
    "\n",
    "# train = pd.DataFrame({\n",
    "#       col:np.repeat(train[col].values, train[\"text\"].str.len())\n",
    "#       for col in train.columns.drop(\"text\")}\n",
    "#     ).assign(**{\"text\":np.concatenate(train[\"text\"].values)})[train.columns]\n",
    "\n",
    "# print(train.iloc[0:0])\n",
    "# print(train.head())\n",
    "# (train.text.apply(pd.Series)\n",
    "#               .stack()\n",
    "#               .reset_index(level=1, drop=True)\n",
    "#               .to_frame('text'))\n",
    "\n",
    "# Convert the author strings into numbers\n",
    "#train['author'] = train['author'].apply(lambda x: possibleAuthors[x])\n",
    "#print(train)\n",
    "\n",
    "# TODO split by sentence\n",
    "#flat_list = [item for sublist in l for item in sublist]\n",
    "#print(train.text.head())\n",
    "\n",
    "text = train['text'].tolist()  # lines\n",
    "#print(train)\n",
    "text = train['sentence'] # sentences\n",
    "author = train['author'].tolist()\n",
    "print(\"there are \", len(text), \"sentences\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectors, author, test_size=0.25, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ngram range is the bounds for ngrams to be extracted\n",
    "here, we do word unigram and bigrams\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=0)\n",
    "vectors = vectorizer.fit_transform(text)\n",
    "print(\"the tf-idf vectors have \", vectors.shape[-1], \"dimensions\")\n",
    "\n",
    "\n",
    "#tvect = TfidfVectorizer(min_df=1, max_df=1)\n",
    "#X_train = tvect.fit_transform(text)\n",
    "\n",
    "min_df is setting thresh for minimal number of \n",
    "\"\"\"\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=0)\n",
    "vectors = vectorizer.fit_transform(text)\n",
    "print(\"the tf-idf vectors have \", vectors.shape[-1], \"dimensions\")\n",
    "\n",
    "\n",
    "### Test the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, author, test_size=0.35, random_state=1337)\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "predictions = svm.predict(X_test)\n",
    "#print(list(predictions[0:10]))\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(y_test, predictions)\n",
    "print(precision, recall, fscore, support)\n",
    "print(\"The Linear SVC model is accurate: \", round((accuracy_score(y_test, predictions) *100),2), \"% of the time.\")\n",
    "\n",
    "# predictions = svm.predict(X_test)\n",
    "# print(list(predictions[0:10]))\n",
    "# print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opEd = pd.read_table(\"OpEd.txt\", header =None)\n",
    "#opEd.columns = ['text']\n",
    "\n",
    "#test = opEd['text'].tolist()\n",
    "\n",
    "\n",
    "with open('OpEd.txt', 'r+') as in_file:\n",
    "    textOpEd = in_file.read()\n",
    "    sentsOpEd = nltk.sent_tokenize(textOpEd)\n",
    "\n",
    "\n",
    "test = pd.DataFrame({'sentence':sentsOpEd})\n",
    "print(test)\n",
    "\n",
    "X_test=vectorizer.transform(test)\n",
    "\n",
    "# testVector = vectorizer.fit_transform(test)\n",
    "# X_test=vectorizer.transform(test)\n",
    "# print(vectors.shape)\n",
    "\n",
    "\n",
    "predictions = svm.predict(X_test)\n",
    "\n",
    "#predictedAuthor = mode(predictions)\n",
    "\n",
    "\n",
    "predictedAuthordf = pd.DataFrame(predictions)\n",
    "predictedAuthordf.columns = ['Author']\n",
    "predictedAuthordf = predictedAuthordf['Author'].value_counts().reset_index()\n",
    "predictedAuthordf = pd.DataFrame(predictedAuthordf)\n",
    "predictedAuthordf.columns = ['Author','Count']\n",
    "predictedAuthordf[\"Probability\"] = predictedAuthordf[\"Count\"]/(predictedAuthordf['Count'].sum())\n",
    "predictedAuthordf[\"logLikelihood\"] = np.log( predictedAuthordf[\"Probability\"])\n",
    "\n",
    "prediction = predictedAuthordf['logLikelihood'].idxmax()\n",
    "predictedAuthor = predictedAuthordf.at[prediction,'Author']\n",
    "\n",
    "print(predictedAuthordf,\"\\n\\n\")\n",
    "\n",
    "\n",
    "#predictedAuthor = (list(possibleAuthors.keys())[list(possibleAuthors.values()).index(predictedAuthor)]) \n",
    "print(\"The predicted author is: \", predictedAuthor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing prediction distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bins = len(set(predictions))\n",
    "\n",
    "plt.hist(predictions, color = \"skyblue\", normed=True, align=\"mid\",bins= bins)\n",
    "plt.xticks(range(bins))\n",
    "plt.ylabel(\"Probability\",fontsize=16)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.title(\"Distribution of Predictions\",fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c = collections.Counter(predictions)\n",
    "c = sorted(c.items())\n",
    "months_num = [i[0] for i in c]\n",
    "freq = [i[1] for i in c]\n",
    "\n",
    "suffixes = []\n",
    "for item in months_num:\n",
    "    suffixes.append(item.split()[-1])\n",
    "    \n",
    "    \n",
    "\n",
    "plt.bar(suffixes, freq)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.ylabel(\"Count\",fontsize=16)\n",
    "plt.title(\"Counts of Predicted Authors\", fontsize=20)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
