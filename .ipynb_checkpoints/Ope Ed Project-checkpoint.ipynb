{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Michael McCormack***\n",
    "\n",
    "In the Week 04 file folder you'll find the anonymous New York op ed from September 5, 2018, together with samples of writings of the main suspects. \n",
    "\n",
    "Use any tools you like to figure out who wrote the op ed. If you can find more writings of these people, please send them my way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How I approached this problem - \n",
    "\n",
    "Let me first say that this is a incredibly interesting problem that demonstrates a very relevant application of NLP.  I had a lot of fun doing this project. \n",
    "\n",
    "I began by doing some exploring on the internet.  I came across this idea of determining author based on sentence structure.  Below is an example of this idea used in a Kaggle competition.\n",
    "\n",
    "https://www.kaggle.com/christopher22/stylometry-identify-authors-by-sentence-structure/notebook\n",
    "\n",
    "Basically the idea is that you break down each training document by sentence and add a label for the author of each sentence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start off by reading in the training files into pandas DataFrames.  I then add a column indicating the author Next I concatenate all the dataframes together so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dan Coats.txt', 'r+') as in_file:\n",
    "    textDan = in_file.read()\n",
    "    sentsDan = nltk.sent_tokenize(textDan)\n",
    "    \n",
    "pdDan = pd.DataFrame({'sentence':sentsDan})\n",
    "pdDan['author'] = 'Dan Coats'\n",
    "\n",
    "\n",
    "with open('James Mattis.txt', 'r+') as in_file:\n",
    "    textMattis = in_file.read()\n",
    "    sentsMattis = nltk.sent_tokenize(textMattis)\n",
    "    \n",
    "pdMattis = pd.DataFrame({'sentence':sentsMattis})\n",
    "pdMattis['author'] = 'James Mattis'\n",
    "\n",
    "\n",
    "with open('John Kelly.txt', 'r+') as in_file:\n",
    "    textKelly = in_file.read()\n",
    "    sentsKelly = nltk.sent_tokenize(textKelly)\n",
    "    \n",
    "pdKelly = pd.DataFrame({'sentence':sentsKelly})\n",
    "pdKelly['author'] = 'John Kelly'\n",
    "\n",
    "\n",
    "with open('Kevin Hassett.txt', 'r+') as in_file:\n",
    "    textHassett = in_file.read()\n",
    "    sentsHassett = nltk.sent_tokenize(textHassett)\n",
    "    \n",
    "pdHassett = pd.DataFrame({'sentence':sentsHassett})\n",
    "pdHassett['author'] = 'Kevin Hassett'\n",
    "\n",
    "with open('Kirstjen Nielsen.txt', 'r+') as in_file:\n",
    "    textNielsen = in_file.read()\n",
    "    sentsNielsen = nltk.sent_tokenize(textNielsen)\n",
    "    \n",
    "pdNielsen = pd.DataFrame({'sentence':sentsNielsen})\n",
    "pdNielsen['author'] = 'Kirstjen Nielsen'\n",
    "\n",
    "with open('Larry Kudlow.txt', 'r+') as in_file:\n",
    "    textKudlow = in_file.read()\n",
    "    sentsKudlow = nltk.sent_tokenize(textKudlow)\n",
    "    \n",
    "pdKudlow = pd.DataFrame({'sentence':sentsKudlow})\n",
    "pdKudlow['author'] = 'Larry Kudlow'\n",
    "\n",
    "\n",
    "with open('Mike Pence.txt', 'r+') as in_file:\n",
    "    textPence = in_file.read()\n",
    "    sentsPence = nltk.sent_tokenize(textPence)\n",
    "    \n",
    "pdPence = pd.DataFrame({'sentence':sentsPence})\n",
    "pdPence['author'] = 'Mike Pence'\n",
    "\n",
    "with open('Mike Pompeo.txt', 'r+') as in_file:\n",
    "    textPompeo = in_file.read()\n",
    "    sentsPompeo = nltk.sent_tokenize(textPompeo)\n",
    "    \n",
    "pdPompeo = pd.DataFrame({'sentence':sentsPompeo})\n",
    "pdPompeo['author'] = 'Mike Pompeo'\n",
    "\n",
    "\n",
    "\n",
    "train = pd.DataFrame()\n",
    "train = pd.concat([pdDan, pdMattis,pdKelly,pdHassett,pdNielsen,pdKudlow,pdPence,pdPompeo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_num ={'Dan Coats': \"Chicken\", 'James Mattis': \"Punk\", \n",
    "                'John Kelly': \"perp\", 'Kevin Hassett': \"Garbage\",\n",
    "                  'Kirstjen Nielsen': \"Scum\", 'Larry Kudlow': \"Toilet\", \n",
    "                'Mike Pence': \"Poop\", 'Mike Pompeo': \"Yuck\"}\n",
    "\n",
    "train[\"author\"].replace(author_to_num, inplace=True)\n",
    "author = train['author'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  1069 sentences\n"
     ]
    }
   ],
   "source": [
    "text = train['sentence'].tolist()\n",
    "\"\"\"\n",
    "get rid of non-breaking spaces,\n",
    "double spaces,\n",
    "NEW LINES\n",
    "other unicode\n",
    "\"\"\"\n",
    "text = [t.replace('\\xa0', ' ').replace(\"\\u2028\", \" \").replace(u'\\ufeff',' ') \\\n",
    "        .replace('\\n', \" \").replace(\"  \", \" \") for t in text]\n",
    "\n",
    "print(\"there are \", len(text), \"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(text)#train['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text, author, test_size=0.25, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ngram range is the bounds for ngrams to be extracted\n",
    "here, we do word unigram and bigrams\n",
    "\n",
    "min_df is setting thresh for minimal number of occurences.\n",
    "\"\"\"\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=0,\n",
    "                            strip_accents=\"unicode\")\n",
    "\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-b7fc37a16c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the tf-idf vectors have \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dimensions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "X_train_vect = vectorizer.transform(X_train)\n",
    "print(\"the tf-idf vectors have \", vectors.shape[-1], \"dimensions\")\n",
    "\n",
    "\n",
    "### Test the model\n",
    "#X_train, X_test, y_train, y_test = train_test_split(vectors, author, test_size=0.35, random_state=1337)\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train_vect, y_train)\n",
    "\n",
    "print(\"testing accuracy: \", svm.score())\n",
    "\n",
    "X_test_vect = vectorizer.fit_transform(X_test)\n",
    "predictions = svm.predict(X_test_vect)\n",
    "#print(list(predictions[0:10]))\n",
    "\n",
    "\n",
    "precision, recall, fscore, support = score(y_test, predictions, average=\"micro\")\n",
    "accuracy = round((accuracy_score(y_test, predictions) *100))\n",
    "\n",
    "print(\"\"\"\n",
    "Model test results for Linear SVC:\n",
    "\n",
    "Precision: {}\n",
    "Recall: {}\n",
    "Accuracy: {}\n",
    "Support: {}\n",
    "\"\"\".format(precision, recall, fscore, support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opEd = pd.read_table(\"OpEd.txt\", header =None)\n",
    "#opEd.columns = ['text']\n",
    "\n",
    "#test = opEd['text'].tolist()\n",
    "\n",
    "\n",
    "with open('OpEd.txt', 'r+') as in_file:\n",
    "    textOpEd = in_file.read()\n",
    "    sentsOpEd = nltk.sent_tokenize(textOpEd)\n",
    "\n",
    "\n",
    "test = pd.DataFrame({'sentence':sentsOpEd})\n",
    "print(test)\n",
    "\n",
    "X_test=vectorizer.transform(test)\n",
    "\n",
    "# testVector = vectorizer.fit_transform(test)\n",
    "# X_test=vectorizer.transform(test)\n",
    "# print(vectors.shape)\n",
    "\n",
    "\n",
    "predictions = svm.predict(X_test)\n",
    "\n",
    "#predictedAuthor = mode(predictions)\n",
    "\n",
    "\n",
    "predictedAuthordf = pd.DataFrame(predictions)\n",
    "predictedAuthordf.columns = ['Author']\n",
    "predictedAuthordf = predictedAuthordf['Author'].value_counts().reset_index()\n",
    "predictedAuthordf = pd.DataFrame(predictedAuthordf)\n",
    "predictedAuthordf.columns = ['Author','Count']\n",
    "predictedAuthordf[\"Probability\"] = predictedAuthordf[\"Count\"]/(predictedAuthordf['Count'].sum())\n",
    "predictedAuthordf[\"logLikelihood\"] = np.log( predictedAuthordf[\"Probability\"])\n",
    "\n",
    "prediction = predictedAuthordf['logLikelihood'].idxmax()\n",
    "predictedAuthor = predictedAuthordf.at[prediction,'Author']\n",
    "\n",
    "print(predictedAuthordf,\"\\n\\n\")\n",
    "\n",
    "\n",
    "#predictedAuthor = (list(possibleAuthors.keys())[list(possibleAuthors.values()).index(predictedAuthor)]) \n",
    "print(\"The predicted author is: \", predictedAuthor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing prediction distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bins = len(set(predictions))\n",
    "\n",
    "plt.hist(predictions, color = \"skyblue\", normed=True, align=\"mid\",bins= bins)\n",
    "plt.xticks(range(bins))\n",
    "plt.ylabel(\"Probability\",fontsize=16)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.title(\"Distribution of Predictions\",fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c = collections.Counter(predictions)\n",
    "c = sorted(c.items())\n",
    "months_num = [i[0] for i in c]\n",
    "freq = [i[1] for i in c]\n",
    "\n",
    "suffixes = []\n",
    "for item in months_num:\n",
    "    suffixes.append(item.split()[-1])\n",
    "    \n",
    "    \n",
    "\n",
    "plt.bar(suffixes, freq)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.ylabel(\"Count\",fontsize=16)\n",
    "plt.title(\"Counts of Predicted Authors\", fontsize=20)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
