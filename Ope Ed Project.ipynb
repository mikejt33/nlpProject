{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Michael McCormack***\n",
    "\n",
    "In the Week 04 file folder you'll find the anonymous New York op ed from September 5, 2018, together with samples of writings of the main suspects. \n",
    "\n",
    "Use any tools you like to figure out who wrote the op ed. If you can find more writings of these people, please send them my way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How I approached this problem - \n",
    "\n",
    "Let me first say that this is a incredibly interesting problem that demonstrates a very relevant application of NLP.  I had a lot of fun doing this project. \n",
    "\n",
    "I began by doing some exploring on the internet.  I came across this idea of determining author based on sentence structure.  Below is an example of this idea used in a Kaggle competition.\n",
    "\n",
    "https://www.kaggle.com/christopher22/stylometry-identify-authors-by-sentence-structure/notebook\n",
    "\n",
    "Basically the idea is that you break down each training document by sentence and add a label for the author of each sentence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start off by reading in the training files into pandas DataFrames.  I then add a column indicating the author Next I concatenate all the dataframes together so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, auc, roc_curve\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dan Coats.txt', 'r+') as in_file:\n",
    "    textDan = in_file.read()\n",
    "    sentsDan = nltk.sent_tokenize(textDan)\n",
    "    \n",
    "pdDan = pd.DataFrame({'sentence':sentsDan})\n",
    "pdDan['author'] = 'Dan Coats'\n",
    "\n",
    "\n",
    "with open('James Mattis.txt', 'r+') as in_file:\n",
    "    textMattis = in_file.read()\n",
    "    sentsMattis = nltk.sent_tokenize(textMattis)\n",
    "    \n",
    "pdMattis = pd.DataFrame({'sentence':sentsMattis})\n",
    "pdMattis['author'] = 'James Mattis'\n",
    "\n",
    "\n",
    "with open('John Kelly.txt', 'r+') as in_file:\n",
    "    textKelly = in_file.read()\n",
    "    sentsKelly = nltk.sent_tokenize(textKelly)\n",
    "    \n",
    "pdKelly = pd.DataFrame({'sentence':sentsKelly})\n",
    "pdKelly['author'] = 'John Kelly'\n",
    "\n",
    "\n",
    "with open('Kevin Hassett.txt', 'r+') as in_file:\n",
    "    textHassett = in_file.read()\n",
    "    sentsHassett = nltk.sent_tokenize(textHassett)\n",
    "    \n",
    "pdHassett = pd.DataFrame({'sentence':sentsHassett})\n",
    "pdHassett['author'] = 'Kevin Hassett'\n",
    "\n",
    "with open('Kirstjen Nielsen.txt', 'r+') as in_file:\n",
    "    textNielsen = in_file.read()\n",
    "    sentsNielsen = nltk.sent_tokenize(textNielsen)\n",
    "    \n",
    "pdNielsen = pd.DataFrame({'sentence':sentsNielsen})\n",
    "pdNielsen['author'] = 'Kirstjen Nielsen'\n",
    "\n",
    "with open('Larry Kudlow.txt', 'r+') as in_file:\n",
    "    textKudlow = in_file.read()\n",
    "    sentsKudlow = nltk.sent_tokenize(textKudlow)\n",
    "    \n",
    "pdKudlow = pd.DataFrame({'sentence':sentsKudlow})\n",
    "pdKudlow['author'] = 'Larry Kudlow'\n",
    "\n",
    "\n",
    "with open('Mike Pence.txt', 'r+') as in_file:\n",
    "    textPence = in_file.read()\n",
    "    sentsPence = nltk.sent_tokenize(textPence)\n",
    "    \n",
    "pdPence = pd.DataFrame({'sentence':sentsPence})\n",
    "pdPence['author'] = 'Mike Pence'\n",
    "\n",
    "with open('Mike Pompeo.txt', 'r+') as in_file:\n",
    "    textPompeo = in_file.read()\n",
    "    sentsPompeo = nltk.sent_tokenize(textPompeo)\n",
    "    \n",
    "pdPompeo = pd.DataFrame({'sentence':sentsPompeo})\n",
    "pdPompeo['author'] = 'Mike Pompeo'\n",
    "\n",
    "\n",
    "\n",
    "train = pd.DataFrame()\n",
    "train = pd.concat([pdDan, pdMattis,pdKelly,pdHassett,pdNielsen,pdKudlow,pdPence,pdPompeo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_num ={'Dan Coats': \"Chicken\", 'James Mattis': \"Punk\", \n",
    "                'John Kelly': \"perp\", 'Kevin Hassett': \"Garbage\",\n",
    "                  'Kirstjen Nielsen': \"Scum\", 'Larry Kudlow': \"Toilet\", \n",
    "                'Mike Pence': \"Poop\", 'Mike Pompeo': \"Yuck\"}\n",
    "\n",
    "train[\"author\"].replace(author_to_num, inplace=True)\n",
    "author = train['author'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  1069 sentences\n"
     ]
    }
   ],
   "source": [
    "text = train['sentence'].tolist()\n",
    "\"\"\"\n",
    "get rid of non-breaking spaces,\n",
    "double spaces,\n",
    "NEW LINES\n",
    "other unicode\n",
    "\"\"\"\n",
    "text = [t.replace('\\xa0', ' ').replace(\"\\u2028\", \" \").replace(u'\\ufeff',' ') \\\n",
    "        .replace('\\n', \" \").replace(\"  \", \" \") for t in text]\n",
    "\n",
    "print(\"there are \", len(text), \"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(text)#train['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text, author, test_size=0.25, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ngram range is the bounds for ngrams to be extracted\n",
    "here, we do word unigram and bigrams\n",
    "\n",
    "min_df is setting thresh for minimal number of occurences.\n",
    "\"\"\"\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=0,\n",
    "                            strip_accents=\"unicode\")\n",
    "\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = vectorizer.transform(X_train)\n",
    "print(\"the tf-idf vectors have \", X_train_vect.shape[-1], \"dimensions\")\n",
    "\n",
    "### Test the model\n",
    "#X_train, X_test, y_train, y_test = train_test_split(vectors, author, test_size=0.35, random_state=1337)\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train_vect, y_train)\n",
    "\n",
    "print(\"testing accuracy: \", svm.loss)\n",
    "\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "predictions = svm.predict(X_test_vect)\n",
    "#print(list(predictions[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Chicken       0.91      0.58      0.71        36\n",
      "    Garbage       0.69      0.77      0.73        56\n",
      "       Poop       1.00      0.75      0.86        16\n",
      "       Punk       0.86      0.67      0.75        27\n",
      "       Scum       1.00      1.00      1.00         8\n",
      "     Toilet       0.72      0.70      0.71        44\n",
      "       Yuck       0.80      0.76      0.78        37\n",
      "       perp       0.64      0.93      0.76        44\n",
      "\n",
      "avg / total       0.78      0.75      0.75       268\n",
      "\n",
      "confusion matrix:\n",
      "[[21  4  0  1  0  1  3  6]\n",
      " [ 1 43  0  0  0  6  0  6]\n",
      " [ 1  1 12  0  0  2  0  0]\n",
      " [ 0  2  0 18  0  0  3  4]\n",
      " [ 0  0  0  0  8  0  0  0]\n",
      " [ 0  7  0  1  0 31  1  4]\n",
      " [ 0  5  0  0  0  1 28  3]\n",
      " [ 0  0  0  1  0  2  0 41]]\n",
      "AUC:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cb492fc4b6f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[1;32m    533\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 534\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    316\u001b[0m     if not (y_type == \"binary\" or\n\u001b[1;32m    317\u001b[0m             (y_type == \"multiclass\" and pos_label is not None)):\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, support = score(y_test, predictions, average=\"macro\")\n",
    "accuracy = round((accuracy_score(y_test, predictions) *100))\n",
    "\n",
    "print(\"classification report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "print(\"AUC:\")\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "print(auc(fpr, tpr))\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "Model test results for Linear SVC:\n",
    "\n",
    "Precision: {}\n",
    "Recall: {}\n",
    "Accuracy: {}\n",
    "Support: {}\n",
    "\"\"\".format(precision, recall, fscore, support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opEd = pd.read_table(\"OpEd.txt\", header =None)\n",
    "#opEd.columns = ['text']\n",
    "\n",
    "#test = opEd['text'].tolist()\n",
    "\n",
    "\n",
    "with open('OpEd.txt', 'r+') as in_file:\n",
    "    textOpEd = in_file.read()\n",
    "    sentsOpEd = nltk.sent_tokenize(textOpEd)\n",
    "\n",
    "\n",
    "test = pd.DataFrame({'sentence':sentsOpEd})\n",
    "print(test)\n",
    "\n",
    "X_test=vectorizer.transform(test)\n",
    "\n",
    "# testVector = vectorizer.fit_transform(test)\n",
    "# X_test=vectorizer.transform(test)\n",
    "# print(vectors.shape)\n",
    "\n",
    "\n",
    "predictions = svm.predict(X_test)\n",
    "\n",
    "#predictedAuthor = mode(predictions)\n",
    "\n",
    "predictedAuthordf = pd.DataFrame(predictions)\n",
    "predictedAuthordf.columns = ['Author']\n",
    "predictedAuthordf = predictedAuthordf['Author'].value_counts().reset_index()\n",
    "predictedAuthordf = pd.DataFrame(predictedAuthordf)\n",
    "predictedAuthordf.columns = ['Author','Count']\n",
    "predictedAuthordf[\"Probability\"] = predictedAuthordf[\"Count\"]/(predictedAuthordf['Count'].sum())\n",
    "predictedAuthordf[\"logLikelihood\"] = np.log( predictedAuthordf[\"Probability\"])\n",
    "\n",
    "prediction = predictedAuthordf['logLikelihood'].idxmax()\n",
    "predictedAuthor = predictedAuthordf.at[prediction,'Author']\n",
    "\n",
    "print(predictedAuthordf,\"\\n\\n\")\n",
    "\n",
    "\n",
    "#predictedAuthor = (list(possibleAuthors.keys())[list(possibleAuthors.values()).index(predictedAuthor)]) \n",
    "print(\"The predicted author is: \", predictedAuthor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing prediction distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bins = len(set(predictions))\n",
    "\n",
    "plt.hist(predictions, color = \"skyblue\", normed=True, align=\"mid\",bins= bins)\n",
    "plt.xticks(range(bins))\n",
    "plt.ylabel(\"Probability\",fontsize=16)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.title(\"Distribution of Predictions\",fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c = collections.Counter(predictions)\n",
    "c = sorted(c.items())\n",
    "months_num = [i[0] for i in c]\n",
    "freq = [i[1] for i in c]\n",
    "\n",
    "suffixes = []\n",
    "for item in months_num:\n",
    "    suffixes.append(item.split()[-1])\n",
    "    \n",
    "    \n",
    "\n",
    "plt.bar(suffixes, freq)\n",
    "plt.xlabel(\"Author\",fontsize=16)\n",
    "plt.ylabel(\"Count\",fontsize=16)\n",
    "plt.title(\"Counts of Predicted Authors\", fontsize=20)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
